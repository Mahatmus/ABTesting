{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "#import pylab\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turning off warnings for neatness\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we demonstrate a few approaches to testing the differences in results, based on different treatments.\n",
    "\n",
    "This is done on an example of data collected by Yandex. The data represents user activity on \"control\" and \"experimental\" versions of the page.\n",
    "\n",
    "Methods used:\n",
    "- Bootstrapping\n",
    "- Basic T-Test\n",
    "- Mann-Whitney test\n",
    "\n",
    "Notebook demonstrates a few important points:\n",
    " - T-Test should only be used after its assumputions are satisfied.\n",
    " - There are methods which allow us to perform testing for data which is not normally distributed.\n",
    " - The fact that there is a significant difference between the sample is **not** enough to make a correct decision, and researched should take care to perform required additional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading data\n",
    "Raw=pd.read_csv(\"ab_browser_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description & Basic Expolration\n",
    "\n",
    "Data comes from an experiment run by Yandex (Russia's most used search engine). The goal of the experiment was to explore if changes to a website affect users actions on the website.\n",
    "\n",
    "Dataset consists of the following variables:\n",
    "- **userID**: user's unique ID\n",
    "- **browser**: browser used by the user\n",
    "- **slot**: control or experimental group\n",
    "- **n_clicks**: number of clicks user made in n_queries\n",
    "- **n_queries**: number of queries a user has run\n",
    "- **n_nonclk_queries**: number of queries where the user did not click any results provided\n",
    "\n",
    "Our goal is to identify if changes in website positively affected the users.\n",
    "\n",
    "**We will concentrate our analysis on n_clicks**, however other variables are certainly to be analyzed at a later stage as well.\n",
    "\n",
    "Additionally, we will try identifiying if there are any additional insights in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Rows in file = %.0f, Columns in file=%.0f\" % Raw.shape)\n",
    "Raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's compare the mean number of clicks between the two groups\n",
    "Total_Clicks_Per_Group=Raw.groupby(\"slot\").n_clicks.aggregate(np.mean)\n",
    "\n",
    "print(\"Mean number of clicks in exp group is %.3f%% higher than in control\" %\n",
    "      ((Total_Clicks_Per_Group[\"exp\"]*1.0/Total_Clicks_Per_Group[\"control\"]-1)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that in experimental group, number of clicks was indeed higher. The difference is not very big - just a couple of percent, but in a digital world it is a lot.\n",
    "\n",
    "If we prove that this difference is significant later, we can state that proposed change is to be recommended for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(Raw.loc[Raw.slot==\"control\",\"n_clicks\"],label=\"Control Group\")\n",
    "sns.kdeplot(Raw.loc[Raw.slot!=\"control\",\"n_clicks\"],label=\"Experimental Group\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the chart, number of clicks distribution is far from normal, with a very heavy right tail. It looks more like Poisson, which is to be expected, given the count nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Bootstrapping will serve two goals:\n",
    "1. Will allow us to test for diffence between the distributions, without paying much attention to its normality\n",
    "2. Allow us to test t-test requirements, to see if we can actually use it\n",
    "\n",
    "\n",
    "### T-test requirements\n",
    "\n",
    "T-test has two basic requirements:\n",
    "1. Sample mean is distributed normally, or approximately normally\n",
    "2. Sample variance estimate is distributed as chi-sq with n-1 degrees of freedom\n",
    "\n",
    "We will test both of those, to see if we could use t-test on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define bootstrapping function, and function to use its results\n",
    "def get_bootstrap_samples(data, n_samples):\n",
    "    data=np.array(data)\n",
    "    indices = np.random.randint(0, len(data), (n_samples, len(data)))\n",
    "    samples = data[indices]\n",
    "    return samples\n",
    "\n",
    "def stat_intervals_from_bootstrap(stat, alpha):\n",
    "    boundaries = np.percentile(stat, [100 * alpha / 2., 100 * (1 - alpha / 2.)])\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "Exp_Group=Raw[Raw.slot==\"exp\"]\n",
    "Control_Group=Raw[Raw.slot==\"control\"]\n",
    "\n",
    "# Create bootstraps\n",
    "Exp_click_bootstraps=get_bootstrap_samples(Exp_Group.n_clicks,500)\n",
    "Control_click_bootstraps=get_bootstrap_samples(Control_Group.n_clicks,500)\n",
    "\n",
    "# Calculate means by group from bootstrapped samples\n",
    "Exp_means=map(np.mean,Exp_click_bootstraps)\n",
    "Control_means=map(np.mean,Control_click_bootstraps)\n",
    "\n",
    "# Calculate bootstrapped differences in means between two groups\n",
    "delta_means = list(map(lambda x: x[1] - x[0], zip(Exp_means, Control_means)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output bootstrapped confidence interval for means\n",
    "print(\"Confidence interval for mean difference : [%.3f, %.3f]\" %\n",
    "      tuple(stat_intervals_from_bootstrap(list(delta_means),0.05)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap mean comparison result\n",
    "As we see, there is no 0 in bootstraped interval for mean difference. As such, we can say already that there is significant difference between the two groups.\n",
    "\n",
    "### T-Test requirements\n",
    "To check T-test requirements, we will use control group, since distributions in the two groups are rather similar. We will get distribution of sample mean and sample variance estimates, and see if they are distributed according to T-test requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Calculate distribution of means and variances\n",
    "Control_means=list(map(np.mean,Control_click_bootstraps))\n",
    "Control_variance=list(map(np.var,Control_click_bootstraps))\n",
    "\n",
    "\n",
    "## Create QQ plots for both distributions, comparing\n",
    "# means with normal\n",
    "# variances with chisq\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "\n",
    "\n",
    "Coords,Fit=stats.probplot(Control_means, dist=\"norm\", plot=axes[0],fit=True)\n",
    "axes[0].set_title('Probability Plot for Means R^2 = %.5f' % (Fit[2]**2))\n",
    "\n",
    "\n",
    "Coords,Fit=stats.probplot(Control_variance, dist=\"chi2\", \n",
    "                     sparams=(len(Control_Group.n_clicks)-1), plot=axes[1],fit=True)\n",
    "axes[1].set_title('Probability Plot for Variances R^2 = %.5f' % (Fit[2]**2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Test requirements results\n",
    "Both based on visual analysis of QQ plots, and R^2 obtained, we can conclude that T-test requirements are satisfied - the distributions are quite close to the ones required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Test\n",
    "\n",
    "Since T-Test requirements are satisfied, we can use this for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(stats.ttest_ind(Exp_Group.n_clicks,Control_Group.n_clicks,equal_var=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, T-Test also shows us that difference between in number of click is signficiant between the samples.\n",
    "\n",
    "## Mann-Whitney test\n",
    "Now, let's pretend that T-Test requirements are not satisfied, and the amount of data is a bit too big to run bootstrapping.\n",
    "A good option in that case would be to use Mann-Whitney test. Its requirements are not as strict as T-test's, and it may be more efficient on a wide range of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.mannwhitneyu(Exp_Group.n_clicks,Control_Group.n_clicks,alternative=\"two-sided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another proof that difference between the two samples is significant (not that we needed another one, but its a learning excersize after all).\n",
    "\n",
    "## Depper dive\n",
    "A poorly-minded data scientist would stop at this point, and say that changes are positive and should be implemented straight away.\n",
    "\n",
    "However, we are better than this. We have another variable in the dataset - browser. Perhaps, it is a good idea to check if the differences are consistent by browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for browser in Raw.browser.unique():\n",
    "    ThisPiece=Raw[Raw.browser==browser]\n",
    "    \n",
    "    Total_Clicks_Per_Group=ThisPiece.groupby(\"slot\").n_clicks.aggregate(np.mean)\n",
    "\n",
    "    print(\"Difference for %s %.3f%% higher than in control\" %\n",
    "          (browser,((Total_Clicks_Per_Group[\"exp\"]*1.0/Total_Clicks_Per_Group[\"control\"]-1)*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! We can see that difference is only really noticeably positive for Browser #14. We have already stated though, that smaller differences may be quite important as well - let's see if they are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for browser in Raw.browser.unique():\n",
    "    ThisPiece=Raw[Raw.browser==browser]\n",
    "    \n",
    "    ThisPieceExp_Group=ThisPiece[ThisPiece.slot==\"exp\"]\n",
    "    ThisPieceControl_Group=ThisPiece[ThisPiece.slot==\"control\"]\n",
    "    \n",
    "    print(\"P-value for Difference for %s is %.3f##\" % \n",
    "        (browser,\n",
    "         stats.mannwhitneyu(ThisPieceExp_Group.n_clicks,ThisPieceControl_Group.n_clicks,alternative=\"two-sided\")[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Method demonstration \n",
    "We have demonstrated usage of three different methods for sig testing AB-experiment results. In this particular case, all of the tests we used were appropriate for the data, and yielded similar results.\n",
    "\n",
    "It is important to notice though, that:\n",
    " - Bootstrapping did take some significant amount of time to run - it might be unwise to run in on sample of such size\n",
    " - We have pointed out that before using T-Test, we need to check if its requirements are satisfied. However, we do not always have an option to run bootstrapping. In this case, it would've been better to check for data normality, and since our data is not normally distributed Mann-Whitney would've been considered a better option.\n",
    " - Thoughtless acceptance of total-level result without subgroup analysis would've potentially had a very negative result - we would've implemented a change which might negatively affect users of one of the browsers\n",
    "\n",
    "### Practical\n",
    "As expected, difference is only significant for Browser 14. Hence, we can only recommed for the change to be implemented for users on this particular browser.\n",
    "\n",
    "However, from a useability point of view, it might be of course confusing for users to have different experiences on different browsers. If that's the case, we would rather implement the change for all of them. But we can see that for Browser #22 there is a negative effect, which is worrysome. If we would rather implement changes for all browsers, it is recommended to collect more sample, to check if for Browser #22 users we do not make the experience worse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
